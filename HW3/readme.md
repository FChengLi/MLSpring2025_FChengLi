### **作业三：图像分类总结报告**

#### **1. 实验概述**

本次作业的核心任务是使用卷积神经网络（CNN）对 Food-11 数据集进行11分类的图像识别。为达成此目标，我分别实现并评估了两种方案：一个是从零开始构建的**自定义CNN（基线模型）**，另一个是基于迁移学习的**预训练ResNet-50（增强版模型）**。本报告旨在对比二者的策略差异，并分析导致其性能巨大差距的关键因素。

#### **2. 实验结果汇总**

通过在 Kaggle 平台上进行训练，两个模型的最终表现与官方评分策略对比如下：

| 模型版本 | Private Score | Public Score | 达成基线 | 训练用时 |
| :--- | ---:| ---:| :--- | :--- |
| 基线模型 (Custom CNN) | 0.60866 | 0.61400 | **Simple** ( > 0.637 ) 未达成 | ~40 分钟 |
| **增强版 (ResNet-50)** | **0.95200** | **0.95533** | **Boss ( > 0.874 )** | **~1 小时** |

实验结果表明，增强版模型不仅轻松超越了最高的“Boss”基线，且训练效率极高，证明了其采用策略的正确性与高效性。

#### **3. 模型对比与性能分析**

增强版模型相较于基线模型，其性能的巨大飞跃源于一系列系统性的优化，核心可归结为以下几点：

**1. 核心策略转变：从零训练 → 迁移学习**
   - **做了什么**：将模型骨干从一个自定义的浅层CNN替换为在ImageNet上预训练好的 **ResNet-50**，并仅替换其最终分类头以适应我们的11类任务。
   - **为何更好**：这是最关键的性能提升点。迁移学习使得模型无需从零学习基础视觉特征（如边缘、纹理、形状），而是直接利用已在百万级图像上学到的强大、通用的特征提取能力，极大地提升了模型的学习起点、收敛速度和最终精度。

**2. 数据预处理的“专业化”**
   - **做了什么**：
     1.  **输入尺寸提升至 224x224**：与ResNet-50的预训练尺寸对齐。
     2.  **加入ImageNet标准化**：使输入数据分布与预训练时保持一致。
     3.  **引入更丰富的数据增强**：如随机翻转、旋转、颜色抖动等。
   - **为何更好**：这些操作确保了预训练权重能够被最大化地有效利用，同时数据增强提升了模型的泛化能力，使其对食物图像中常见的视角、光照变化更具鲁棒性。

**3. 训练过程的“智能化”**
   - **做了什么**：
     1.  **引入学习率调度器 (`ReduceLROnPlateau`)**：在验证集损失停滞时自动降低学习率。
     2.  **设置更合理的早停机制 (`patience=5`)**：避免无效训练，防止过拟合。
   - **为何更好**：实现了训练过程的自动精细化调优，帮助模型跳出局部最优，从而达到更高的精度，并显著提升了训练效率。

**4. 模型诊断与可解释性 (t-SNE 可视化)**
   - **做了什么**：通过 t-SNE 对 ResNet-50 中层 (`layer3`) 和顶层 (`avgpool`) 的特征进行降维可视化。
   - **分析结论**:
     - **中层特征 (`tsne_mid`)**：类别边界模糊，大量数据点混杂在一起，表明此时的特征主要关注局部纹理和形状，语义区分度不足。
     - **顶层特征 (`tsne_top`)**：形成了多个边界清晰、内部紧凑的类别簇，证明经过模型深层处理后，特征已具备了强大的语义可分性。
   - **为何更好**：t-SNE 可视化直观地**验证了迁移学习的有效性**，展示了模型从提取通用特征到形成高级语义表征的学习过程。同时，它也指出了部分类别（中心区域靠近的簇）依然存在混淆，为下一步的精细优化提供了方向。

#### **4. 实验总结**

本次作业的成功关键在于**从“从零搭建”到“站在巨人肩膀上”的思维转变**。基线CNN证明了基础网络结构的可行性，而增强版通过**系统性地应用迁移学习的最佳实践**——包括使用预训练的 ResNet-50、匹配其输入规范、应用强大的数据增强和智能的训练策略——最终以极高的效率实现了远超目标的分数。t-SNE 的分析也为模型的黑盒行为提供了一扇观察窗口，使优化过程更为有据可循。